{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Examples\n",
    "# Refere to notebook/LLM_inference_perf.ipynb and notebook/LLM_memory_analysis.ipynb to get familiar with the setup.\n",
    "# 请参考 notebook/LLM_inference_perf.ipynb 和 notebook/LLM_memory_analysis.ipynb 以熟悉设置。\n",
    "# import os, sys, warnings\n",
    "# script_dir = os.getcwd()\n",
    "# module_path = script_dir\n",
    "# for _ in range(1):\n",
    "#     module_path = os.path.abspath(os.path.join(module_path, '../'))\n",
    "#     if module_path not in sys.path:\n",
    "#         sys.path.insert(0,module_path)\n",
    "        \n",
    "# from src import decode_moddeling, prefill_moddeling\n",
    "# import pandas as pd\n",
    "# from plotnine import *\n",
    "# import plotnine as p9\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from Systems.system_configs import *\n",
    "# All_model_list = ['opt_125m', 'opt_350m', 'opt_1b', 'opt_175b', 'gemma_7b', 'LLaMA_7b', 'llama3_8b',  'llama_13b', 'mixtral_7x8',  'LLaMA_70b', 'dbrx', 'grok-1', 'gpt-3',  'gpt-4']\n",
    "# All_models_name = ['facebook/opt-125m', 'facebook/opt-350m', 'facebook/opt-1.3b', 'facebook/opt-175b', 'google/gemma-7b', 'meta-llama/Llama-2-7b', 'meta-llama/Meta-Llama-3', 'meta-llama/Llama-2-13b', 'mistralai/Mixtral-8x7B', 'meta-llama/Llama-2-70b', 'databricks/dbrx-base', 'xai-org/grok-1', 'openai/gpt-3', 'openai/gpt-4']\n",
    "\n",
    "# 导入必要的模块\n",
    "import os, sys, warnings\n",
    "\n",
    "# 获取当前工作目录\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# 将模块路径设置为当前工作目录\n",
    "module_path = script_dir\n",
    "\n",
    "# 向上导航一级目录（重复1次）\n",
    "for _ in range(1):\n",
    "    module_path = os.path.abspath(os.path.join(module_path, '../'))\n",
    "\n",
    "# 如果模块路径不在系统路径中，则将其插入到路径的开头\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "# 从'src'包中导入特定模块\n",
    "from src import decode_moddeling, prefill_moddeling\n",
    "\n",
    "# 导入 pandas 用于数据处理\n",
    "import pandas as pd\n",
    "\n",
    "# 导入 plotnine 用于绘图\n",
    "from plotnine import *\n",
    "\n",
    "# 使用 p9 别名引用 plotnine\n",
    "import plotnine as p9\n",
    "\n",
    "# 导入 tqdm 用于显示进度条\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 从 'Systems.system_configs' 导入系统配置\n",
    "from Systems.system_configs import *\n",
    "\n",
    "# 定义模型名称列表和对应的标识符\n",
    "All_model_list = ['opt_125m', 'opt_350m', 'opt_1b', 'opt_175b', 'gemma_7b', 'LLaMA_7b', 'llama3_8b', 'llama_13b', 'mixtral_7x8', 'LLaMA_70b', 'dbrx', 'grok-1', 'gpt-3', 'gpt-4']\n",
    "All_models_name = ['facebook/opt-125m', 'facebook/opt-350m', 'facebook/opt-1.3b', 'facebook/opt-175b', 'google/gemma-7b', 'meta-llama/Llama-2-7b', 'meta-llama/Meta-Llama-3', 'meta-llama/Llama-2-13b', 'mistralai/Mixtral-8x7B', 'meta-llama/Llama-2-70b', 'databricks/dbrx-base', 'xai-org/grok-1', 'openai/gpt-3', 'openai/gpt-4']\n",
    "\n",
    "# 这两个列表定义了模型的名称和对应的标识符。下面逐一详解每个模型：\n",
    "\n",
    "# opt_125m: 模型名称为 'opt_125m'，对应的标识符为 'facebook/opt-125m'。这可能指代一个 Facebook 公开的 125 million 参数的优化模型。\n",
    "\n",
    "# opt_350m: 模型名称为 'opt_350m'，对应的标识符为 'facebook/opt-350m'。这可能指代一个 Facebook 公开的 350 million 参数的优化模型。\n",
    "\n",
    "# opt_1b: 模型名称为 'opt_1b'，对应的标识符为 'facebook/opt-1.3b'。这可能指代一个 Facebook 公开的 1.3 billion 参数的优化模型。\n",
    "\n",
    "# opt_175b: 模型名称为 'opt_175b'，对应的标识符为 'facebook/opt-175b'。这可能指代一个 Facebook 公开的 175 billion 参数的优化模型。\n",
    "\n",
    "# gemma_7b: 模型名称为 'gemma_7b'，对应的标识符为 'google/gemma-7b'。这可能指代一个 Google 公开的 7 billion 参数的 GPT 模型。\n",
    "\n",
    "# LLaMA_7b: 模型名称为 'LLaMA_7b'，对应的标识符为 'meta-llama/Llama-2-7b'。这可能指代 Meta LLaMA 项目的 7 billion 参数的 LLaMA-2 模型。\n",
    "\n",
    "# llama3_8b: 模型名称为 'llama3_8b'，对应的标识符为 'meta-llama/Meta-Llama-3'。这可能指代 Meta LLaMA 项目的 8 billion 参数的 Meta-Llama-3 模型。\n",
    "\n",
    "# llama_13b: 模型名称为 'llama_13b'，对应的标识符为 'meta-llama/Llama-2-13b'。这可能指代 Meta LLaMA 项目的 13 billion 参数的 LLaMA-2 模型。\n",
    "\n",
    "# mixtral_7x8: 模型名称为 'mixtral_7x8'，对应的标识符为 'mistralai/Mixtral-8x7B'。这可能指代 Mistral AI 公开的 8x7 billion 参数的 Mixtral 模型。\n",
    "\n",
    "# LLaMA_70b: 模型名称为 'LLaMA_70b'，对应的标识符为 'meta-llama/Llama-2-70b'。这可能指代 Meta LLaMA 项目的 70 billion 参数的 LLaMA-2 模型。\n",
    "\n",
    "# dbrx: 模型名称为 'dbrx'，对应的标识符为 'databricks/dbrx-base'。这可能指代 Databricks 提供的基础模型。\n",
    "\n",
    "# grok-1: 模型名称为 'grok-1'，对应的标识符为 'xai-org/grok-1'。这可能指代 XAI-ORG 提供的第一个 Grok 模型。\n",
    "\n",
    "# gpt-3: 模型名称为 'gpt-3'，对应的标识符为 'openai/gpt-3'。这可能指代 OpenAI 提供的 GPT-3 模型。\n",
    "\n",
    "# gpt-4: 模型名称为 'gpt-4'，对应的标识符为 'openai/gpt-4'。这可能指代 OpenAI 提供的 GPT-4 模型。\n",
    "\n",
    "# 这些模型名称和标识符通常用于区分和引用不同的预训练语言模型，每个模型可能具有不同的参数量和用途\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e134144703274b018028048c1fa802dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='System:', index=2, options=('A100_40GB_GPU', 'A100_80GB_GPU', 'H10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.generate_demand_curve(system_box, num_nodes_slider, model_box, quantization_box, batch_slider, beam_size, input_token_slider, output_token_slider)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotnine import *\n",
    "import plotnine as p9\n",
    "\n",
    "\n",
    "# Set up interactive widgets for the variables\n",
    "from ipywidgets import interact, IntSlider, Checkbox, BoundedIntText, BoundedFloatText, Dropdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Define the function to generate the demand curve\n",
    "def generate_demand_curve(system_box, num_nodes_slider, model_box, quantization_box, batch_slider, beam_size, input_token_slider, output_token_slider):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    system_box= globals()[system_box]\n",
    "    data = []\n",
    "    batch_size = int(batch_slider)\n",
    "    for model in tqdm(model_box):\n",
    "            prefill_df, prefill_summary_table = prefill_moddeling(model = model, batch_size = batch_size,\n",
    "                                    input_tokens = input_token_slider, output_tokens = output_token_slider, \n",
    "                                    system_name = system_box,\n",
    "                                    bits=quantization_box, model_profilling=True,\n",
    "                                    tensor_parallel = num_nodes_slider)\n",
    "            total_memory = int(system_box.get('Memory_size'))*1024\n",
    "            memory_left = total_memory - prefill_summary_table['Model Weights (MB)'].values[0]\n",
    "            per_token_prefill_kv_cache = prefill_summary_table['KV Cache (MB)'].values[0] * beam_size / input_token_slider\n",
    "            data.append([model,'Prefill',batch_size, input_token_slider, output_token_slider] + list(prefill_summary_table.loc[0].values) + [int(memory_left/per_token_prefill_kv_cache)])\n",
    "            decode_df , decode_summary_table = decode_moddeling(model = model, batch_size = batch_size, Bb = beam_size ,\n",
    "                                    input_tokens = input_token_slider, output_tokens = output_token_slider, \n",
    "                                    system_name = system_box,\n",
    "                                    bits=quantization_box, model_profilling=True,\n",
    "                                    tensor_parallel = num_nodes_slider) \n",
    "            data.append([model,'Decode',batch_size, input_token_slider, output_token_slider] + list(decode_summary_table.loc[0].values) + [int(memory_left/per_token_prefill_kv_cache - output_token_slider )])\n",
    "    assert len(data) > 0, \"No Model fits in the given # of GPUs. Increase GPUs or use different Model\"\n",
    "    data_df = pd.DataFrame(data, columns = ['Model', 'Stage','Batch', 'Input Context Length', 'Num Output Tokens'] + list(prefill_summary_table.columns) + ['Max Tokens Possible'])\n",
    "    data_df = data_df.replace(All_model_list, All_models_name)\n",
    "    data_df['Stage'] = pd.Categorical(data_df['Stage'], categories=['Prefill','Decode'])\n",
    "\n",
    "    data_df.rename(columns={'Model Weights (MB)': 'Weights per Node(MB)', 'KV Cache (MB)': 'KV Cache per Node(MB)'}, inplace=True)\n",
    "\n",
    "    display(data_df[['Model', 'Stage', 'Batch', 'Input Context Length', 'Num Output Tokens', 'Weights per Node(MB)', 'KV Cache per Node(MB)', 'Max Tokens Possible']])\n",
    "\n",
    "\n",
    "\n",
    "batch_slider = widgets.Text( value='8', description='Batch Size:', disabled=False , style={'description_width': 'initial'})\n",
    "beam_size = widgets.IntSlider(value=1, min=1, max=16, description='# of Parallel Beams:', style={'description_width': 'initial'},)\n",
    "input_token_slider = BoundedIntText( value=512, min=1, max= 100000, step=1, description='Input Tokens:', disabled=False , style={'description_width': 'initial'})\n",
    "output_token_slider = BoundedIntText( value=128, min=1, max= 100000, step=1, description='Output Tokens:', disabled=False , style={'description_width': 'initial'})\n",
    "\n",
    "quantization_box = Dropdown( options=['bf16', 'int8', 'int4'], value='int8', description='Quantization:', disabled=False , style={'description_width': 'initial'},)\n",
    "model_box = widgets.SelectMultiple( options=[\n",
    "    ('facebook/opt-125m','opt_125m'),\n",
    "    ('facebook/opt-350m','opt_350m'),\n",
    "    ('facebook/opt-1.3b','opt_1b'),\n",
    "    ('facebook/opt-175b','opt_175b'),\n",
    "    ('google/gemma-7b','gemma_7b'),\n",
    "    ('meta-llama/Llama-2-7b','LLaMA_7b'),\n",
    "    ('meta-llama/Meta-Llama-3-8B','llama3_8b'), \n",
    "    ('meta-llama/Llama-2-13b','llama_13b'),\n",
    "    ('mistralai/Mixtral-8x7B','mixtral_7x8'), \n",
    "    ('meta-llama/Llama-2-70b','LLaMA_70b'),\n",
    "    ('databricks/dbrx-base','dbrx'),\n",
    "    ('xai-org/grok-1','grok-1'),\n",
    "    ('openai/gpt-3','gpt-3'), \n",
    "    ('openai/gpt-4','gpt-4')\n",
    "    ], value=['opt_125m'], description='Models:', disabled=False,)\n",
    "system_box = Dropdown( options=['A100_40GB_GPU', 'A100_80GB_GPU', 'H100_GPU','GH200_GPU', 'TPUv4','TPUv5e', 'MI300X', 'Gaudi3'], value='H100_GPU', description='System:', disabled=False,)\n",
    "num_nodes_slider = BoundedIntText( value=2, min=1, max=128, step=1, description='# Nodes:', disabled=False)\n",
    "\n",
    "\n",
    "# Create an interactive plot\n",
    "interact(generate_demand_curve,\n",
    "         system_box=system_box, num_nodes_slider=num_nodes_slider, model_box=model_box, quantization_box=quantization_box,\n",
    "         batch_slider=batch_slider, beam_size = beam_size, input_token_slider=input_token_slider, output_token_slider=output_token_slider, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fffa5c8de24c6c8727a04b614c2929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='System:', index=2, options=('A100_40GB_GPU', 'A100_80GB_GPU', 'H10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.generate_demand_curve(system_box, num_nodes_slider, model_box, quantization_box, batch_slider, beam_size, input_token_slider, output_token_slider)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotnine import *\n",
    "import plotnine as p9\n",
    "\n",
    "# 设置交互式小部件变量\n",
    "from ipywidgets import interact, IntSlider, Checkbox, BoundedIntText, BoundedFloatText, Dropdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# 定义生成需求曲线的函数\n",
    "def generate_demand_curve(system_box, num_nodes_slider, model_box, quantization_box, batch_slider, beam_size, input_token_slider, output_token_slider):\n",
    "    warnings.filterwarnings(\"ignore\")  # 忽略警告\n",
    "    system_box = globals()[system_box]  # 获取全局变量中的 system_box\n",
    "    data = []  # 初始化数据列表\n",
    "    batch_size = int(batch_slider)  # 将 batch_slider 转换为整数并赋值给 batch_size\n",
    "    for model in tqdm(model_box):  # 遍历 model_box 中的模型\n",
    "        # 调用 prefill_modelling 函数生成 prefill 数据和总结表\n",
    "        prefill_df, prefill_summary_table = prefill_modelling(model=model, batch_size=batch_size,\n",
    "                                    input_tokens=input_token_slider, output_tokens=output_token_slider, \n",
    "                                    system_name=system_box,\n",
    "                                    bits=quantization_box, model_profilling=True,\n",
    "                                    tensor_parallel=num_nodes_slider)\n",
    "        total_memory = int(system_box.get('Memory_size')) * 1024  # 获取系统总内存并转换为 MB\n",
    "        memory_left = total_memory - prefill_summary_table['Model Weights (MB)'].values[0]  # 计算剩余内存\n",
    "        per_token_prefill_kv_cache = prefill_summary_table['KV Cache (MB)'].values[0] * beam_size / input_token_slider  # 计算每个 token 的 KV 缓存\n",
    "        # 添加 prefill 阶段数据到 data 列表\n",
    "        data.append([model, 'Prefill', batch_size, input_token_slider, output_token_slider] + \n",
    "                    list(prefill_summary_table.loc[0].values) + [int(memory_left / per_token_prefill_kv_cache)])\n",
    "        # 调用 decode_modelling 函数生成 decode 数据和总结表\n",
    "        decode_df, decode_summary_table = decode_modelling(model=model, batch_size=batch_size, Bb=beam_size,\n",
    "                                    input_tokens=input_token_slider, output_tokens=output_token_slider, \n",
    "                                    system_name=system_box,\n",
    "                                    bits=quantization_box, model_profilling=True,\n",
    "                                    tensor_parallel=num_nodes_slider)\n",
    "        # 添加 decode 阶段数据到 data 列表\n",
    "        data.append([model, 'Decode', batch_size, input_token_slider, output_token_slider] + \n",
    "                    list(decode_summary_table.loc[0].values) + \n",
    "                    [int(memory_left / per_token_prefill_kv_cache - output_token_slider)])\n",
    "    assert len(data) > 0, \"No Model fits in the given # of GPUs. Increase GPUs or use different Model\"  # 确保数据不为空\n",
    "    # 创建 DataFrame 并添加列名\n",
    "    data_df = pd.DataFrame(data, columns=['Model', 'Stage', 'Batch', 'Input Context Length', 'Num Output Tokens'] + \n",
    "                           list(prefill_summary_table.columns) + ['Max Tokens Possible'])\n",
    "    # 替换模型名称\n",
    "    data_df = data_df.replace(All_model_list, All_models_name)\n",
    "    data_df['Stage'] = pd.Categorical(data_df['Stage'], categories=['Prefill', 'Decode'])  # 将 Stage 列转换为类别变量\n",
    "\n",
    "    # 重命名列名\n",
    "    data_df.rename(columns={'Model Weights (MB)': 'Weights per Node(MB)', 'KV Cache (MB)': 'KV Cache per Node(MB)'}, inplace=True)\n",
    "\n",
    "    # 显示最终的 DataFrame\n",
    "    display(data_df[['Model', 'Stage', 'Batch', 'Input Context Length', 'Num Output Tokens', \n",
    "                     'Weights per Node(MB)', 'KV Cache per Node(MB)', 'Max Tokens Possible']])\n",
    "batch_slider = widgets.Text(\n",
    "    value='8', \n",
    "    description='Batch Size:', \n",
    "    disabled=False, \n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "# 创建一个文本输入小部件，用于设置批次大小，默认值为 '8'，可以编辑。\n",
    "\n",
    "beam_size = widgets.IntSlider(\n",
    "    value=1, \n",
    "    min=1, \n",
    "    max=16, \n",
    "    description='# of Parallel Beams:', \n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "# 创建一个整数滑动条小部件，用于设置并行束的数量，默认值为 1，范围从 1 到 16。\n",
    "\n",
    "input_token_slider = BoundedIntText(\n",
    "    value=512, \n",
    "    min=1, \n",
    "    max=100000, \n",
    "    step=1, \n",
    "    description='Input Tokens:', \n",
    "    disabled=False, \n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "# 创建一个有界整数文本输入小部件，用于设置输入 token 的数量，默认值为 512，范围从 1 到 100000。\n",
    "\n",
    "output_token_slider = BoundedIntText(\n",
    "    value=128, \n",
    "    min=1, \n",
    "    max=100000, \n",
    "    step=1, \n",
    "    description='Output Tokens:', \n",
    "    disabled=False, \n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "# 创建一个有界整数文本输入小部件，用于设置输出 token 的数量，默认值为 128，范围从 1 到 100000。\n",
    "\n",
    "quantization_box = Dropdown(\n",
    "    options=['bf16', 'int8', 'int4'], \n",
    "    value='int8', \n",
    "    description='Quantization:', \n",
    "    disabled=False, \n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "# 创建一个下拉菜单小部件，用于选择量化方式，选项包括 'bf16', 'int8', 'int4'，默认值为 'int8'。\n",
    "\n",
    "model_box = widgets.SelectMultiple(\n",
    "    options=[\n",
    "        ('facebook/opt-125m','opt_125m'),\n",
    "        ('facebook/opt-350m','opt_350m'),\n",
    "        ('facebook/opt-1.3b','opt_1b'),\n",
    "        ('facebook/opt-175b','opt_175b'),\n",
    "        ('google/gemma-7b','gemma_7b'),\n",
    "        ('meta-llama/Llama-2-7b','LLaMA_7b'),\n",
    "        ('meta-llama/Meta-Llama-3-8B','llama3_8b'), \n",
    "        ('meta-llama/Llama-2-13b','llama_13b'),\n",
    "        ('mistralai/Mixtral-8x7B','mixtral_7x8'), \n",
    "        ('meta-llama/Llama-2-70b','LLaMA_70b'),\n",
    "        ('databricks/dbrx-base','dbrx'),\n",
    "        ('xai-org/grok-1','grok-1'),\n",
    "        ('openai/gpt-3','gpt-3'), \n",
    "        ('openai/gpt-4','gpt-4')\n",
    "    ], \n",
    "    value=['opt_125m'], \n",
    "    description='Models:', \n",
    "    disabled=False\n",
    ")\n",
    "# 创建一个多选下拉菜单小部件，用于选择模型，提供了一些模型选项，默认选中 'opt_125m'。\n",
    "\n",
    "system_box = Dropdown(\n",
    "    options=['A100_40GB_GPU', 'A100_80GB_GPU', 'H100_GPU','GH200_GPU', 'TPUv4','TPUv5e', 'MI300X', 'Gaudi3'], \n",
    "    value='H100_GPU', \n",
    "    description='System:', \n",
    "    disabled=False\n",
    ")\n",
    "# 创建一个下拉菜单小部件，用于选择系统配置，选项包括多种 GPU 和 TPU，默认值为 'H100_GPU'。\n",
    "\n",
    "num_nodes_slider = BoundedIntText(\n",
    "    value=2, \n",
    "    min=1, \n",
    "    max=128, \n",
    "    step=1, \n",
    "    description='# Nodes:', \n",
    "    disabled=False\n",
    ")\n",
    "# 创建一个有界整数文本输入小部件，用于设置节点数量，默认值为 2，范围从 1 到 128。\n",
    "\n",
    "# 创建一个交互式图表\n",
    "interact(\n",
    "    generate_demand_curve,\n",
    "    system_box=system_box, \n",
    "    num_nodes_slider=num_nodes_slider, \n",
    "    model_box=model_box, \n",
    "    quantization_box=quantization_box,\n",
    "    batch_slider=batch_slider, \n",
    "    beam_size=beam_size, \n",
    "    input_token_slider=input_token_slider, \n",
    "    output_token_slider=output_token_slider\n",
    ")\n",
    "# 使用 interact 函数创建交互式图表，传入 generate_demand_curve 函数及其参数。\n",
    "\n",
    "# system_box:\n",
    "\n",
    "# 类型: 字符串\n",
    "# 描述: 表示所选的系统配置，例如不同型号的 GPU 或 TPU。这个参数用于确定运行模型时所使用的硬件配置。\n",
    "# num_nodes_slider:\n",
    "\n",
    "# 类型: 整数\n",
    "# 描述: 表示所使用的计算节点数量。每个节点通常代表一个物理或虚拟计算单元（例如一个 GPU 实例）。\n",
    "# model_box:\n",
    "\n",
    "# 类型: 列表（包含字符串）\n",
    "# 描述: 表示所选择的一个或多个模型名称。这些模型将用于生成需求曲线，每个模型都会单独进行分析。\n",
    "# quantization_box:\n",
    "\n",
    "# 类型: 字符串\n",
    "# 描述: 表示所选的量化方法，例如 'bf16'、'int8'、'int4' 等。量化方法用于减少模型的内存和计算需求，通过降低数值精度来优化性能。\n",
    "# batch_slider:\n",
    "\n",
    "# 类型: 字符串\n",
    "# 描述: 表示批次大小（batch size），即一次输入到模型中的样本数量。通常用于控制训练或推理时的并行处理能力。\n",
    "# beam_size:\n",
    "\n",
    "# 类型: 整数\n",
    "# 描述: 表示并行束的数量（beam size），主要用于生成任务中的束搜索算法。束搜索是一种广泛用于自然语言处理任务的搜索策略。\n",
    "# input_token_slider:\n",
    "\n",
    "# 类型: 整数\n",
    "# 描述: 表示输入 token 的数量。token 是自然语言处理中的基本单位，可以是单词、子词或字符。这个参数用于确定输入序列的长度。\n",
    "# output_token_slider:\n",
    "\n",
    "# 类型: 整数\n",
    "# 描述: 表示输出 token 的数量。与输入 token 类似，表示模型生成的输出序列的长度。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genz_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "script_dir = os.getcwd()\n",
    "module_path = script_dir\n",
    "for _ in range(1):\n",
    "    module_path = os.path.abspath(os.path.join(module_path, '../'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.insert(0,module_path)\n",
    "        \n",
    "from src import decode_moddeling, prefill_moddeling\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "import plotnine as p9\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Systems.system_configs import *\n",
    "All_model_list = ['opt_125m', 'opt_350m', 'opt_1b', 'opt_175b', 'gemma_7b', 'LLaMA_7b', 'llama3_8b',  'llama_13b', 'mixtral_7x8',  'LLaMA_70b', 'dbrx', 'grok-1', 'gpt-3',  'gpt-4']\n",
    "All_models_name = ['facebook/opt-125m', 'facebook/opt-350m', 'facebook/opt-1.3b', 'facebook/opt-175b', 'google/gemma-7b', 'meta-llama/Llama-2-7b', 'meta-llama/Meta-Llama-3', 'meta-llama/Llama-2-13b', 'mistralai/Mixtral-8x7B', 'meta-llama/Llama-2-70b', 'databricks/dbrx-base', 'xai-org/grok-1', 'openai/gpt-3', 'openai/gpt-4']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea94a62679044c86845186c6fdbd9d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='System:', index=2, options=('A100_40GB_GPU', 'A100_80GB_GPU', 'H10â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.generate_demand_curve(system_box, num_nodes_slider, model_box, quantization_box, batch_slider, beam_size, input_token_slider, output_token_slider)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotnine import *\n",
    "import plotnine as p9\n",
    "\n",
    "\n",
    "# Set up interactive widgets for the variables\n",
    "from ipywidgets import interact, IntSlider, Checkbox, BoundedIntText, BoundedFloatText, Dropdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Define the function to generate the demand curve\n",
    "def generate_demand_curve(system_box, num_nodes_slider, model_box, quantization_box, batch_slider, beam_size, input_token_slider, output_token_slider):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    system_box= globals()[system_box]\n",
    "    data = []\n",
    "    batch_size = int(batch_slider)\n",
    "    for model in tqdm(model_box):\n",
    "            prefill_df, prefill_summary_table = prefill_moddeling(model = model, batch_size = batch_size,\n",
    "                                    input_tokens = input_token_slider, output_tokens = output_token_slider, \n",
    "                                    system_name = system_box,\n",
    "                                    bits=quantization_box, model_profilling=True,\n",
    "                                    tensor_parallel = num_nodes_slider)\n",
    "            total_memory = int(system_box.get('Memory_size'))*1024\n",
    "            memory_left = total_memory - prefill_summary_table['Model Weights (MB)'].values[0]\n",
    "            per_token_prefill_kv_cache = prefill_summary_table['KV Cache (MB)'].values[0] * beam_size / input_token_slider\n",
    "            data.append([model,'Prefill',batch_size, input_token_slider, output_token_slider] + list(prefill_summary_table.loc[0].values) + [int(memory_left/per_token_prefill_kv_cache)])\n",
    "            decode_df , decode_summary_table = decode_moddeling(model = model, batch_size = batch_size, Bb = beam_size ,\n",
    "                                    input_tokens = input_token_slider, output_tokens = output_token_slider, \n",
    "                                    system_name = system_box,\n",
    "                                    bits=quantization_box, model_profilling=True,\n",
    "                                    tensor_parallel = num_nodes_slider) \n",
    "            data.append([model,'Decode',batch_size, input_token_slider, output_token_slider] + list(decode_summary_table.loc[0].values) + [int(memory_left/per_token_prefill_kv_cache - output_token_slider )])\n",
    "    assert len(data) > 0, \"No Model fits in the given # of GPUs. Increase GPUs or use different Model\"\n",
    "    data_df = pd.DataFrame(data, columns = ['Model', 'Stage','Batch', 'Input Context Length', 'Num Output Tokens'] + list(prefill_summary_table.columns) + ['Max Tokens Possible'])\n",
    "    data_df = data_df.replace(All_model_list, All_models_name)\n",
    "    data_df['Stage'] = pd.Categorical(data_df['Stage'], categories=['Prefill','Decode'])\n",
    "\n",
    "    data_df.rename(columns={'Model Weights (MB)': 'Weights per Node(MB)', 'KV Cache (MB)': 'KV Cache per Node(MB)'}, inplace=True)\n",
    "\n",
    "    display(data_df[['Model', 'Stage', 'Batch', 'Input Context Length', 'Num Output Tokens', 'Weights per Node(MB)', 'KV Cache per Node(MB)', 'Max Tokens Possible']])\n",
    "\n",
    "\n",
    "\n",
    "batch_slider = widgets.Text( value='8', description='Batch Size:', disabled=False , style={'description_width': 'initial'})\n",
    "beam_size = widgets.IntSlider(value=1, min=1, max=16, description='# of Parallel Beams:', style={'description_width': 'initial'},)\n",
    "input_token_slider = BoundedIntText( value=512, min=1, max= 100000, step=1, description='Input Tokens:', disabled=False , style={'description_width': 'initial'})\n",
    "output_token_slider = BoundedIntText( value=128, min=1, max= 100000, step=1, description='Output Tokens:', disabled=False , style={'description_width': 'initial'})\n",
    "\n",
    "quantization_box = Dropdown( options=['bf16', 'int8', 'int4'], value='int8', description='Quantization:', disabled=False , style={'description_width': 'initial'},)\n",
    "model_box = widgets.SelectMultiple( options=[\n",
    "    ('facebook/opt-125m','opt_125m'),\n",
    "    ('facebook/opt-350m','opt_350m'),\n",
    "    ('facebook/opt-1.3b','opt_1b'),\n",
    "    ('facebook/opt-175b','opt_175b'),\n",
    "    ('google/gemma-7b','gemma_7b'),\n",
    "    ('meta-llama/Llama-2-7b','LLaMA_7b'),\n",
    "    ('meta-llama/Meta-Llama-3-8B','llama3_8b'), \n",
    "    ('meta-llama/Llama-2-13b','llama_13b'),\n",
    "    ('mistralai/Mixtral-8x7B','mixtral_7x8'), \n",
    "    ('meta-llama/Llama-2-70b','LLaMA_70b'),\n",
    "    ('databricks/dbrx-base','dbrx'),\n",
    "    ('xai-org/grok-1','grok-1'),\n",
    "    ('openai/gpt-3','gpt-3'), \n",
    "    ('openai/gpt-4','gpt-4')\n",
    "    ], value=['opt_125m'], description='Models:', disabled=False,)\n",
    "system_box = Dropdown( options=['A100_40GB_GPU', 'A100_80GB_GPU', 'H100_GPU','GH200_GPU', 'TPUv4','TPUv5e', 'MI300X', 'Gaudi3'], value='H100_GPU', description='System:', disabled=False,)\n",
    "num_nodes_slider = BoundedIntText( value=2, min=1, max=128, step=1, description='# Nodes:', disabled=False)\n",
    "\n",
    "\n",
    "# Create an interactive plot\n",
    "interact(generate_demand_curve,\n",
    "         system_box=system_box, num_nodes_slider=num_nodes_slider, model_box=model_box, quantization_box=quantization_box,\n",
    "         batch_slider=batch_slider, beam_size = beam_size, input_token_slider=input_token_slider, output_token_slider=output_token_slider, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genz_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
